{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check your versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.19.1\n",
      "pandas 1.1.1\n",
      "scipy 1.5.2\n",
      "sklearn 0.23.2\n",
      "lightgbm 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy.sparse \n",
    "import lightgbm \n",
    "\n",
    "for p in [np, pd, scipy, sklearn, lightgbm]:\n",
    "    print (p.__name__, p.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!** There is a huge chance that the assignment will be impossible to pass if the versions of `lighgbm` and `scikit-learn` are wrong. The versions being tested:\n",
    "\n",
    "    numpy 1.13.1\n",
    "    pandas 0.20.3\n",
    "    scipy 0.19.1\n",
    "    sklearn 0.19.0\n",
    "    ligthgbm 2.0.6\n",
    "    \n",
    "\n",
    "To install an older version of `lighgbm` you may use the following command:\n",
    "```\n",
    "pip uninstall lightgbm\n",
    "pip install lightgbm==2.0.6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking.\n",
    "\n",
    "We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 20)\n",
    "# pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "# from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the hard drive first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('../readonly/final_project_data/sales_train.csv.gz')\n",
    "shops = pd.read_csv('../readonly/final_project_data/shops.csv')\n",
    "items = pd.read_csv('../readonly/final_project_data/items.csv')\n",
    "item_cats = pd.read_csv('../readonly/final_project_data/item_categories.csv')\n",
    "tests = pd.read_csv('../readonly/final_project_data/test.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  4,  5,  6,  7, 10, 12, 14, 15, 16, 18, 19, 21, 22, 24, 25,\n",
       "       26, 28, 31, 34, 35, 36, 37, 38, 39, 41, 42, 44, 45, 46, 47, 48, 49,\n",
       "       50, 52, 53, 55, 56, 57, 58, 59], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(\n",
    "    tests['shop_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And use only 3 shops for simplicity.\n",
    "# sales = sales[sales['shop_id'].isin([26, 27, 28])]\n",
    "# tests = tests[tests['shop_id'].isin([26, 27, 28])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "sales = sales[sales['item_price']<100000]\n",
    "sales = sales[sales['item_cnt_day']<1001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the features. This part is all implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Append test date to our grid\n",
    "tests['date_block_num'] = 34\n",
    "grid = grid.append(tests.drop(columns=['ID']), ignore_index=True)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n",
    "gb.rename(columns={'item_cnt_day':'target'}, inplace=True)\n",
    "\n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "gb.rename(columns={'item_cnt_day':'target_shop'}, inplace=True)\n",
    "                                                                      \n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "all_data\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n",
    "gb.rename(columns={'item_cnt_day':'target_item'}, inplace=True)\n",
    "\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Downcast dtypes from 64 to 32 bit to save memory\n",
    "all_data = downcast_dtypes(all_data)\n",
    "del grid, gb \n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of \"all_data\":', all_data.shape[0], 'rows,', all_data.shape[1], 'columns')\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_parquet('all_data_first.parquet', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_parquet('all_data_first.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data_first.copy()\n",
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in tqdm(shift_range):\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    \n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "\n",
    "# Don't use old data from year 2013\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "print('to_drop_cols =', to_drop_cols)\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n",
    "all_data = downcast_dtypes(all_data)\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we've created a feature matrix. It is stored in `all_data` variable. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.head(5)\n",
    "all_data.to_parquet('all_data.parquet', index=True)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data[all_data['date_block_num']==34]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sake of the programming assignment, let's artificially split the data into train and test. We will treat last month data as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_parquet('all_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 33\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "last_block = dates.max()-1\n",
    "print('Test `date_block_num` is %d' % last_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_train = dates[dates <  last_block]\n",
    "dates_test  = dates[dates == last_block]\n",
    "\n",
    "X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "\n",
    "y_train = all_data.loc[dates <  last_block, 'target'].values\n",
    "y_test =  all_data.loc[dates == last_block, 'target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>target_lag_1</th>\n",
       "      <th>target_item_lag_1</th>\n",
       "      <th>target_shop_lag_1</th>\n",
       "      <th>target_lag_2</th>\n",
       "      <th>target_item_lag_2</th>\n",
       "      <th>target_shop_lag_2</th>\n",
       "      <th>target_lag_3</th>\n",
       "      <th>target_item_lag_3</th>\n",
       "      <th>target_shop_lag_3</th>\n",
       "      <th>target_lag_4</th>\n",
       "      <th>target_item_lag_4</th>\n",
       "      <th>target_shop_lag_4</th>\n",
       "      <th>target_lag_5</th>\n",
       "      <th>target_item_lag_5</th>\n",
       "      <th>target_shop_lag_5</th>\n",
       "      <th>target_lag_12</th>\n",
       "      <th>target_item_lag_12</th>\n",
       "      <th>target_shop_lag_12</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "      <td>10297</td>\n",
       "      <td>3.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>10296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>10298</td>\n",
       "      <td>21.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1309.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>10300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>10284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10055.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6676.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7827.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6186917</th>\n",
       "      <td>27</td>\n",
       "      <td>21279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3357.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3786.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6186918</th>\n",
       "      <td>27</td>\n",
       "      <td>21283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3357.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3518.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4026.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6186919</th>\n",
       "      <td>27</td>\n",
       "      <td>21352</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2478.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3786.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4026.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6186920</th>\n",
       "      <td>27</td>\n",
       "      <td>21284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3357.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6186921</th>\n",
       "      <td>27</td>\n",
       "      <td>21105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6186922 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         shop_id  item_id  target_lag_1  target_item_lag_1  target_shop_lag_1  \\\n",
       "index                                                                           \n",
       "0             54    10297           3.0               42.0            10055.0   \n",
       "1             54    10296           0.0               24.0            10055.0   \n",
       "2             54    10298          21.0              369.0            10055.0   \n",
       "3             54    10300           1.0               54.0            10055.0   \n",
       "4             54    10284           0.0                4.0            10055.0   \n",
       "...          ...      ...           ...                ...                ...   \n",
       "6186917       27    21279           0.0                0.0                0.0   \n",
       "6186918       27    21283           0.0                0.0                0.0   \n",
       "6186919       27    21352           0.0                0.0                0.0   \n",
       "6186920       27    21284           0.0                0.0                0.0   \n",
       "6186921       27    21105           0.0                0.0                0.0   \n",
       "\n",
       "         target_lag_2  target_item_lag_2  target_shop_lag_2  target_lag_3  \\\n",
       "index                                                                       \n",
       "0                 0.0                2.0             7978.0           0.0   \n",
       "1                 0.0                0.0                0.0           0.0   \n",
       "2               119.0             1309.0             7978.0           7.0   \n",
       "3                31.0              361.0             7978.0           0.0   \n",
       "4                 0.0                3.0             7978.0           0.0   \n",
       "...               ...                ...                ...           ...   \n",
       "6186917           0.0                0.0                0.0           1.0   \n",
       "6186918           0.0                0.0                0.0           1.0   \n",
       "6186919           1.0                2.0             2478.0           0.0   \n",
       "6186920           0.0                0.0                0.0           0.0   \n",
       "6186921           0.0                0.0                0.0           0.0   \n",
       "\n",
       "         target_item_lag_3  target_shop_lag_3  target_lag_4  \\\n",
       "index                                                         \n",
       "0                      0.0                0.0           0.0   \n",
       "1                      0.0                0.0           0.0   \n",
       "2                    144.0             6676.0           0.0   \n",
       "3                     53.0             6676.0           0.0   \n",
       "4                      5.0             6676.0           0.0   \n",
       "...                    ...                ...           ...   \n",
       "6186917                1.0             3357.0           1.0   \n",
       "6186918                1.0             3357.0           0.0   \n",
       "6186919                0.0                0.0           1.0   \n",
       "6186920                1.0             3357.0           0.0   \n",
       "6186921                0.0                0.0           0.0   \n",
       "\n",
       "         target_item_lag_4  target_shop_lag_4  target_lag_5  \\\n",
       "index                                                         \n",
       "0                      0.0                0.0           0.0   \n",
       "1                      0.0                0.0           0.0   \n",
       "2                      0.0                0.0           0.0   \n",
       "3                      0.0                0.0           0.0   \n",
       "4                      3.0             7827.0           0.0   \n",
       "...                    ...                ...           ...   \n",
       "6186917                1.0             3786.0           0.0   \n",
       "6186918                0.0                0.0           0.0   \n",
       "6186919                1.0             3786.0           0.0   \n",
       "6186920                0.0                0.0           0.0   \n",
       "6186921                0.0                0.0           0.0   \n",
       "\n",
       "         target_item_lag_5  target_shop_lag_5  target_lag_12  \\\n",
       "index                                                          \n",
       "0                      0.0                0.0            0.0   \n",
       "1                      0.0                0.0            0.0   \n",
       "2                      0.0                0.0            0.0   \n",
       "3                      0.0                0.0            0.0   \n",
       "4                     10.0             7792.0            0.0   \n",
       "...                    ...                ...            ...   \n",
       "6186917                0.0                0.0            0.0   \n",
       "6186918                1.0             3518.0            0.0   \n",
       "6186919                0.0                0.0            0.0   \n",
       "6186920                0.0                0.0            0.0   \n",
       "6186921                0.0                0.0            0.0   \n",
       "\n",
       "         target_item_lag_12  target_shop_lag_12  item_category_id  \n",
       "index                                                              \n",
       "0                       0.0                 0.0                37  \n",
       "1                       0.0                 0.0                38  \n",
       "2                       0.0                 0.0                40  \n",
       "3                       0.0                 0.0                37  \n",
       "4                       0.0                 0.0                57  \n",
       "...                     ...                 ...               ...  \n",
       "6186917                 0.0                 0.0                61  \n",
       "6186918                 4.0              4026.0                61  \n",
       "6186919                 2.0              4026.0                37  \n",
       "6186920                 0.0                 0.0                61  \n",
       "6186921                 0.0                 0.0                61  \n",
       "\n",
       "[6186922 rows x 21 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First level models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement a basic stacking scheme. We have a time component here, so we will use ***scheme f)*** from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let's see how we get test meta-features first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firts, we will run *linear regression* on numeric columns and get predictions for the last month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for linreg is 0.481407\n",
      "Test RMSE linreg is 1.941356\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train.values, y_train)\n",
    "pred_lr = lr.predict(X_test.values)\n",
    "\n",
    "print('Test R-squared for linreg is %f' % r2_score(y_test, pred_lr))\n",
    "print('Test RMSE linreg is %f' % np.sqrt(mean_squared_error(y_test, pred_lr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the we run *LightGBM*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for LightGBM is 0.473683\n",
      "Test RMSE LightGBM is 1.955761\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':1, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0,\n",
    "               'num_threads': 4\n",
    "              }\n",
    "\n",
    "model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb = model.predict(X_test)\n",
    "\n",
    "print('Test R-squared for LightGBM is %f' % r2_score(y_test, pred_lgb))\n",
    "print('Test RMSE LightGBM is %f' % np.sqrt(mean_squared_error(y_test, pred_lgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.129634\n",
      "0:\tlearn: 2.5996040\ttest: 2.5996040\tbest: 2.5996040 (0)\ttotal: 213ms\tremaining: 3m 32s\n",
      "50:\tlearn: 1.3114095\ttest: 1.3145556\tbest: 1.3145556 (50)\ttotal: 8.38s\tremaining: 2m 35s\n",
      "100:\tlearn: 1.1745542\ttest: 1.1777186\tbest: 1.1777186 (100)\ttotal: 17.3s\tremaining: 2m 34s\n",
      "150:\tlearn: 1.1372048\ttest: 1.1383621\tbest: 1.1383621 (150)\ttotal: 25.7s\tremaining: 2m 24s\n",
      "200:\tlearn: 1.1057551\ttest: 1.1075768\tbest: 1.1075768 (200)\ttotal: 34.2s\tremaining: 2m 15s\n",
      "250:\tlearn: 1.0798676\ttest: 1.0873038\tbest: 1.0873038 (250)\ttotal: 42.7s\tremaining: 2m 7s\n",
      "300:\tlearn: 1.0587525\ttest: 1.0683456\tbest: 1.0683456 (300)\ttotal: 50.8s\tremaining: 1m 57s\n",
      "350:\tlearn: 1.0424496\ttest: 1.0535275\tbest: 1.0535275 (350)\ttotal: 59.6s\tremaining: 1m 50s\n",
      "400:\tlearn: 1.0295295\ttest: 1.0425684\tbest: 1.0425684 (400)\ttotal: 1m 8s\tremaining: 1m 43s\n",
      "450:\tlearn: 1.0179669\ttest: 1.0346224\tbest: 1.0346224 (450)\ttotal: 1m 18s\tremaining: 1m 35s\n",
      "500:\tlearn: 1.0057138\ttest: 1.0249135\tbest: 1.0248198 (499)\ttotal: 1m 26s\tremaining: 1m 26s\n",
      "550:\tlearn: 0.9866411\ttest: 1.0116016\tbest: 1.0114819 (549)\ttotal: 1m 35s\tremaining: 1m 17s\n",
      "600:\tlearn: 0.9744697\ttest: 1.0033040\tbest: 1.0032967 (598)\ttotal: 1m 45s\tremaining: 1m 9s\n",
      "650:\tlearn: 0.9649808\ttest: 0.9962715\tbest: 0.9962351 (649)\ttotal: 1m 54s\tremaining: 1m 1s\n",
      "700:\tlearn: 0.9556117\ttest: 0.9887864\tbest: 0.9887864 (700)\ttotal: 2m 3s\tremaining: 52.6s\n",
      "750:\tlearn: 0.9456841\ttest: 0.9822365\tbest: 0.9822349 (749)\ttotal: 2m 12s\tremaining: 44s\n",
      "800:\tlearn: 0.9370511\ttest: 0.9757034\tbest: 0.9757034 (800)\ttotal: 2m 21s\tremaining: 35.1s\n",
      "850:\tlearn: 0.9289058\ttest: 0.9727746\tbest: 0.9727746 (850)\ttotal: 2m 31s\tremaining: 26.5s\n",
      "900:\tlearn: 0.9239533\ttest: 0.9697979\tbest: 0.9697963 (899)\ttotal: 2m 40s\tremaining: 17.7s\n",
      "950:\tlearn: 0.9199420\ttest: 0.9679321\tbest: 0.9678238 (948)\ttotal: 2m 49s\tremaining: 8.71s\n",
      "999:\tlearn: 0.9143714\ttest: 0.9651486\tbest: 0.9650937 (997)\ttotal: 2m 58s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9650937329\n",
      "bestIteration = 997\n",
      "\n",
      "Shrink model to first 998 iterations.\n",
      "Test R-squared for CatBoost is 0.871839\n",
      "Test RMSE CatBoost is 0.965094\n"
     ]
    }
   ],
   "source": [
    "# from catboost import CatBoostRegressor, Pool\n",
    "import catboost as cb\n",
    "\n",
    "cat_features = ['shop_id', 'item_id', 'item_category_id']\n",
    "cb_train = cb.Pool(X_train, label=y_train, cat_features=cat_features)\n",
    "cb_test = cb.Pool(X_test, label=y_test, cat_features=cat_features)\n",
    "\n",
    "cbr = cb.CatBoostRegressor(loss_function='RMSE')\n",
    "cbr.fit(cb_test, eval_set=cb_test, verbose=100)\n",
    "pred_cb = cbr.predict(X_test)\n",
    "\n",
    "print('Test R-squared for CatBoost is %f' % r2_score(y_test, pred_cb))\n",
    "print('Test RMSE CatBoost is %f' % np.sqrt(mean_squared_error(y_test, pred_cb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:52:05] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[11:52:05] WARNING: src/learner.cc:686: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[0]\tvalidation_0-rmse:3.40077\tvalidation_1-rmse:2.60711\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 10 rounds.\n",
      "[10]\tvalidation_0-rmse:2.89193\tvalidation_1-rmse:2.14024\n",
      "[20]\tvalidation_0-rmse:2.75055\tvalidation_1-rmse:2.05922\n",
      "[30]\tvalidation_0-rmse:2.68437\tvalidation_1-rmse:2.02856\n",
      "[40]\tvalidation_0-rmse:2.63867\tvalidation_1-rmse:2.01844\n",
      "[50]\tvalidation_0-rmse:2.60148\tvalidation_1-rmse:2.01308\n",
      "[60]\tvalidation_0-rmse:2.58247\tvalidation_1-rmse:2.00714\n",
      "[70]\tvalidation_0-rmse:2.56999\tvalidation_1-rmse:2.00407\n",
      "[80]\tvalidation_0-rmse:2.55968\tvalidation_1-rmse:2.00099\n",
      "[90]\tvalidation_0-rmse:2.55306\tvalidation_1-rmse:2.00069\n",
      "Stopping. Best iteration:\n",
      "[84]\tvalidation_0-rmse:2.55626\tvalidation_1-rmse:2.00016\n",
      "\n",
      "Test R-squared for XGBoost is 0.449423\n",
      "Test RMSE XGBoost is 2.000328\n"
     ]
    }
   ],
   "source": [
    "# XGBM\n",
    "import xgboost as xgb\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(n_estimators=1000, n_jobs=4, random_state=17)\n",
    "model_xgb.fit(X_train, y_train, \n",
    "              eval_metric=\"rmse\", \n",
    "              eval_set=[(X_train, y_train), (X_test, y_test)], \n",
    "              verbose=10, \n",
    "              early_stopping_rounds = 30)\n",
    "pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "print('Test R-squared for XGBoost is %f' % r2_score(y_test, pred_xgb))\n",
    "print('Test RMSE XGBoost is %f' % np.sqrt(mean_squared_error(y_test, pred_xgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, concatenate test predictions to get test meta-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_level2 = np.c_[pred_lr, pred_lgb, pred_cb, pred_xgb] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.12520302, 0.05498366, 0.11109322, 0.12267822],\n",
       "        [0.77716301, 0.79737357, 0.97165807, 0.77547413],\n",
       "        [0.99906431, 0.50825388, 0.64431465, 0.45457107],\n",
       "        ...,\n",
       "        [0.10154384, 0.2267074 , 0.06519398, 0.19298005],\n",
       "        [0.12411467, 0.06156986, 0.08642288, 0.16359174],\n",
       "        [0.05064061, 0.04538967, 0.05318729, 0.07221007]]),\n",
       " (238172, 4))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_level2, X_test_level2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now it is your turn to write the code**. You need to implement ***scheme f)*** from the reading material. Here, we will use duration **T** equal to month and **M=15**.  \n",
    "\n",
    "That is, you need to get predictions (meta-features) from *linear regression* and *LightGBM* for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_level2, X_train_level2.shape, y_train_level2, y_train_level2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be77021a377494488fb88ff8ba17a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0.31640568 0.29015536]\n"
     ]
    }
   ],
   "source": [
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], y_train_level2.shape[1]])\n",
    "\n",
    "M=15\n",
    "\n",
    "X_list_2 = []\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in tqdm([27, 28, 29, 30, 31, 32]):\n",
    "    \n",
    "    #print(cur_block_num)\n",
    "    \n",
    "    '''\n",
    "        1. Split `X_train` into parts\n",
    "           Remember, that corresponding dates are stored in `dates_train` \n",
    "        2. Fit linear regression \n",
    "        3. Fit LightGBM and put predictions          \n",
    "        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n",
    "           You can use `dates_train_level2` for it\n",
    "           Make sure the order of the meta-features is the same as in `X_test_level2`\n",
    "    '''      \n",
    "    \n",
    "    #  YOUR CODE GOES HERE\n",
    "    cdates = (dates <  cur_block_num)\n",
    "    cX_test = all_data.loc[cdates].drop(to_drop_cols, axis=1)\n",
    "    cy_train = all_data.loc[cdates, 'target'].values\n",
    "    cX_pred = all_data.loc[dates==cur_block_num].drop(to_drop_cols, axis=1)\n",
    "\n",
    "    lr_c = LinearRegression()\n",
    "    lr_c.fit(cX_test.values, cy_train)\n",
    "    pred_lr_c = lr_c.predict(cX_pred.values)\n",
    "\n",
    "    model_lgb_c = lgb.train(lgb_params, lgb.Dataset(cX_test, label=cy_train), 100)\n",
    "    pred_lgb_c = model_lgb_c.predict(cX_pred)\n",
    "    X_list_2.append(np.c_[pred_lr_c, pred_lgb_c])\n",
    "    \n",
    "X_train_level2 = np.vstack(X_list_2)\n",
    "print(X_train_level2.mean(axis=0))\n",
    "# Sanity check\n",
    "# assert np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988,  1.38811989]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig *scatter plot* between the two metafeatures. Plot the scatter plot below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "plt.scatter(X_train_level2[:,0], X_train_level2[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the meta-features are created, we can ensemble our first level models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple convex mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with simple linear convex mix:\n",
    "\n",
    "$$\n",
    "mix= \\alpha\\cdot\\text{linreg_prediction}+(1-\\alpha)\\cdot\\text{lgb_prediction}\n",
    "$$\n",
    "\n",
    "We need to find an optimal $\\alpha$. And it is very easy, as it is feasible to do grid search. Next, find the optimal $\\alpha$ out of `alphas_to_try` array. Remember, that you need to use train meta-features (not test) when searching for $\\alpha$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_to_try = np.linspace(0, 1, 1001)\n",
    "\n",
    "best_score = 0\n",
    "# best_alpha = 0\n",
    "\n",
    "for alpha in alphas_to_try:\n",
    "    mix = alpha*X_train_level2[:,0] + (1-alpha)*X_train_level2[:,1]\n",
    "    #np.c_[X_train_level2, mix]\n",
    "    score = r2_score(y_train_level2, mix)\n",
    "    # print(score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_alpha = alpha\n",
    "        best_mix = mix\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "best_alpha = best_alpha # YOUR CODE GOES HERE\n",
    "r2_train_simple_mix = best_score # YOUR CODE GOES HERE\n",
    "\n",
    "print('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, r2_train_simple_mix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the $\\alpha$ you've found to compute predictions for the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = best_alpha*X_test_level2[:,0] + (1-best_alpha)*X_test_level2[:,1] # YOUR CODE GOES HERE\n",
    "r2_test_simple_mix = r2_score(y_test, test_preds) # YOUR CODE GOES HERE\n",
    "\n",
    "print('Test R-squared for simple mix is %f' % r2_test_simple_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "lr_meta = LinearRegression()\n",
    "# X_train_l2_meta = np.c_[X_train_level2, best_mix]\n",
    "# X_test_l2_meta = np.c_[X_test_level2, test_preds]\n",
    "\n",
    "lr_meta.fit(X_train_level2, y_train_level2)\n",
    "# pred_lr_meta = lr_meta.predict(X_test_l2_meta)\n",
    "# print('Test R-squared for linreg is %f' % r2_score(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute R-squared on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = lr_meta.predict(X_train_level2) # YOUR CODE GOES HERE\n",
    "r2_train_stacking = r2_score(y_train_level2, train_preds) # YOUR CODE GOES HERE\n",
    "\n",
    "test_preds = lr_meta.predict(X_test_level2) # YOUR CODE GOES HERE\n",
    "r2_test_stacking = r2_score(y_test, test_preds) # YOUR CODE GOES HERE\n",
    "\n",
    "print('Train R-squared for stacking is %f' % r2_train_stacking)\n",
    "print('Test  R-squared for stacking is %f' % r2_test_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. **Examine and compare** train and test scores for the two methods. \n",
    "\n",
    "And of course this particular case does not mean simple mix is always better than stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all done! Submit everything we need to the grader now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader\n",
    "grader = Grader()\n",
    "\n",
    "grader.submit_tag('best_alpha', best_alpha)\n",
    "\n",
    "grader.submit_tag('r2_train_simple_mix', r2_train_simple_mix)\n",
    "grader.submit_tag('r2_test_simple_mix',  r2_test_simple_mix)\n",
    "\n",
    "grader.submit_tag('r2_train_stacking', r2_train_stacking)\n",
    "grader.submit_tag('r2_test_stacking',  r2_test_stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_EMAIL = 'ddementiev@yandex.ru' # EMAIL HERE\n",
    "STUDENT_TOKEN = '9o3OQup3ZKX2Ryho' # TOKEN HERE\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
